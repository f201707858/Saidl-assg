{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGICAL WAY AND YOU CAN DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def to_categorical(y, num_classes=None):\\n#len function is not applicable for list so we convert it into an array\\n    y = np.array(y, dtype='int')\\n    n = y.shape[0]\\n    hot_encodings = np.zeros((n,n))\\n    for i in range(n):\\n        hot_encodings[y[i],i] = 1\\n    return hot_encodings,y,n\\ny = [2,5,7,8,6,9,4,0,1,3]\\n\\nprint(to_categorical(y))\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def to_categorical(y, num_classes=None):\n",
    "#len function is not applicable for list so we convert it into an array\n",
    "    y = np.array(y, dtype='int')\n",
    "    n = y.shape[0]\n",
    "    hot_encodings = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        hot_encodings[y[i],i] = 1\n",
    "    return hot_encodings,y,n\n",
    "y = [2,5,7,8,6,9,4,0,1,3]\n",
    "\n",
    "print(to_categorical(y))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    def predict( context , W1, W2):\n",
    "       # Predict output from input data and weights\n",
    "        :param x: input data\n",
    "        :param W1: weights from input to hidden layer\n",
    "        :param W2: weights from hidden layer to output layer\n",
    "        :return: output of neural network#\n",
    "        \n",
    "        h = np.mean([np.dot(W1.T, xx) for xx in x], axis=0)\n",
    "        u = np.dot(W2.T, h)\n",
    "        return softmax(u)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love urself before loving anyone else\n"
     ]
    }
   ],
   "source": [
    "print(\"love urself before loving anyone else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r', encoding=\"utf8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    \n",
    "    #works to be done : More cleaning using spacy\n",
    "    # tokens = word_tokenize(doc)\n",
    "    tokens =doc.split()\n",
    "    \n",
    "    tokens= [word for word in tokens if word.isalpha()]\n",
    "    tokens = [words.lower() for words in tokens]\n",
    "    \n",
    "    #using nltk lib\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [words for words in tokens if words not in stop_words]\n",
    "    tokens = [words for words in tokens if len(words) > 2] # vectorization\n",
    "    return tokens\n",
    "\n",
    "def add_vocab_to_bow(filename ,vocab):\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "#but  point is you have list of files not a single file\n",
    "def entire_folder1(directory , vocab ) :\n",
    "    for filename in listdir(directory) :#list  dir gives the list \n",
    "        path = directory + '/' + filename\n",
    "        add_vocab_to_bow(path, vocab)\n",
    "        \n",
    "        \n",
    "vocab = Counter()#VOCAB IS NOT A FILE\n",
    "entire_folder1('C:/Users/pc/Downloads/aclImdb/train/pos', vocab)\n",
    "entire_folder1('C:/Users/pc/Downloads/aclImdb/train/neg', vocab)\n",
    "entire_folder1('C:/Users/pc/Downloads/aclImdb/train/unsup', vocab)\n",
    "\n",
    "#make a file with all the words thereby removing infrequent words\n",
    "min_freq = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_freq]\n",
    "\n",
    "def save_file(tokens,filename):\n",
    "    #one word in each file\n",
    "    data = '\\n'.join (tokens)\n",
    "    file = open(filename,'w', encoding=\"utf8\")\n",
    "    file.write(data)\n",
    "    file.close()    \n",
    "save_file(tokens,'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making the folder ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cartoon', 'comedy'], ['ran', 'time', 'programs', 'school'], ['years', 'teaching', 'profession', 'lead', 'believe', 'satire', 'much', 'closer', 'reality'], ['scramble', 'survive', 'insightful', 'students', 'see', 'right', 'pathetic', 'pettiness', 'whole', 'remind', 'schools', 'knew', 'students'], ['saw', 'episode', 'student', 'repeatedly', 'tried', 'burn', 'immediately', 'recalled'], ['classic', 'sack', 'one', 'teachers'], ['expect', 'many', 'adults', 'age', 'think', 'far', 'fetched'], ['pity'], ['given', 'really', 'well', 'done', 'special', 'story'], ['personal', 'love', 'person', 'one', 'fine', 'usual', 'superb', 'self'], ['well', 'casting'], ['enjoy', 'story', 'real', 'people', 'real', 'love', 'winner'], ['much'], ['good'], ['unique', 'interesting', 'arrangement'], ['absence', 'violence', 'porno', 'sex', 'refreshing'], ['convincing', 'felt', 'like', 'could', 'understand', 'feelings'], ['enjoyable', 'movie'], ['much'], ['good'], ['unique', 'interesting', 'arrangement'], ['absence', 'violence', 'sex', 'refreshing'], ['convincing', 'felt', 'like', 'could', 'understand', 'feelings'], ['enjoyable', 'movie'], ['production', 'authentic', 'locale', 'lush', 'score', 'resulted', 'star', 'collectors', 'item'], ['got', 'passable', 'star', 'mostly', 'memorable', 'tried'], ['bring', 'art', 'house', 'style', 'film', 'mainstream'], ['small', 'town', 'locale', 'story', 'ordinary', 'people', 'genre', 'well', 'satisfy', 'grownups'], ['unable', 'hide', 'braininess', 'enough', 'make', 'character', 'believable'], ['wondered', 'post', 'doctorate', 'instead', 'working', 'dead', 'end', 'factory', 'job'], ['character', 'bit', 'contrived'], ['nice', 'guy', 'loser', 'turns', 'actually', 'little', 'help', 'version'], ['genre', 'successfully', 'handled', 'mid'], ['wish', 'main', 'stream', 'studios', 'would', 'try', 'stuff', 'post', 'adolescents', 'reserve', 'couple', 'screens', 'multi', 'cinema', 'complexes', 'efforts'], ['give', 'effort'], ['film', 'special', 'place', 'caught', 'first', 'teaching', 'adult', 'literacy'], ['rang', 'true', 'even', 'outstanding', 'student', 'time'], ['scenes', 'make', 'gulp', 'sudden', 'even', 'put', 'smile', 'face', 'sheer', 'identification', 'characters', 'situation'], ['performances', 'rank', 'best', 'great', 'turn', 'young', 'inspiring', 'story', 'haunting', 'musical', 'score', 'makes', 'enjoyable', 'rewarding', 'experience'], ['guess', 'film', 'need', 'fluid', 'seamless'], ['skip', 'background', 'fast', 'slow', 'etc'], ['scene', 'library'], ['many', 'minor', 'flaws', 'yet', 'detract', 'overall', 'positive', 'impact', 'watching', 'people', 'help', 'areas', 'life', 'seem', 'hardest', 'fix'], ['characters', 'smart'], ['understand', 'enough', 'function', 'read', 'much', 'adventure', 'childhood'], ['although', 'enough', 'adventure', 'understand', 'move', 'past', 'life', 'took'], ['faults', 'characters', 'compliment'], ['may', 'bit', 'stretch', 'accept', 'would', 'wind', 'working', 'year', 'year', 'never', 'hid', 'illiteracy', 'enough', 'work', 'construction', 'job'], ['explained', 'course', 'unfolding', 'seems', 'somewhat', 'contrived'], ['assume', 'one', 'took', 'time', 'rethink', 'script'], ['good', 'imagine', 'would', 'done', 'screen', 'someone'], ['found', 'nice', 'ending', 'generally', 'nice', 'feel'], ['family', 'film', 'considered', 'behavior', 'considering', 'stars', 'would', 'expected', 'lot', 'rougher', 'least'], ['memorable', 'part', 'film', 'portrayal', 'difficult', 'must', 'learn', 'read', 'write', 'already', 'adult'], ['big', 'theme', 'movie', 'involves', 'touching', 'scenes', 'film', 'memorable'], ['still', 'fairly', 'nice', 'tale', 'would', 'happy', 'recommend'], ['complex', 'film', 'explores', 'effects', 'modes', 'industrial', 'capitalist', 'production', 'human', 'relations'], ['constant', 'references', 'assembly', 'line', 'workers', 'treated', 'cogs', 'overseen', 'managers', 'wielding', 'controlling', 'much', 'hair', 'workers', 'leave', 'firing', 'workers', 'meet', 'criteria', 'supervisor', 'always', 'hard', 'good', 'may', 'unspecified', 'future', 'make', 'mistake'], ['system', 'destroys', 'families', 'send', 'father', 'nursing', 'home', 'quickly', 'loses', 'job'], ['daughter', 'single', 'teen', 'mother', 'drops', 'high', 'school', 'take', 'job', 'plant'], ['made', 'fact', 'declining', 'partners', 'need', 'implication', 'nobody', 'left', 'home', 'care', 'kids'], ['husband', 'dead', 'multiple', 'references', 'film', 'costs', 'medical', 'viewer', 'must', 'wonder', 'might', 'lived', 'better', 'costly', 'care'], ['brother', 'law', 'gets', 'abusive', 'yet', 'another', 'unsuccessful', 'day', 'unemployment', 'office', 'wife', 'yells', 'buying', 'beer', 'savings', 'instead', 'leaving', 'face', 'lift', 'teeth', 'job', 'working', 'class', 'stake', 'conventional', 'bourgeois', 'notions', 'perfection', 'beauty', 'buy'], ['one', 'reference', 'race', 'film', 'black', 'factory', 'line', 'worker', 'whose', 'husband', 'jail', 'also', 'black', 'men', 'suffer', 'high', 'incarceration'], ['remarks', 'like', 'family', 'composed', 'prisoner', 'wage', 'slave'], ['still', 'believes', 'human', 'relations', 'therefore', 'film', 'outside', 'system', 'capitalism'], ['cares', 'father', 'spite', 'fact', 'traveling', 'salesman', 'job', 'resulted', 'illiteracy', 'yet', 'reduced', 'human', 'relations', 'purely', 'instrumental', 'brother', 'law', 'wrong'], ['conform', 'routine', 'everyone', 'uses', 'technology', 'techniques', 'industrial', 'production', 'creative', 'sort', 'ideal'], ['dream', 'early', 'socialists', 'use', 'technology', 'provide', 'basic', 'allowing', 'free', 'time', 'creative', 'human', 'work', 'fuller', 'human', 'relations'], ['also', 'outside', 'traditional', 'gender', 'relations'], ['cares', 'knows', 'iron'], ['lives', 'traditionally', 'male', 'role', 'factory', 'mains', 'source', 'income', 'brings', 'public', 'traditionally', 'women'], ['teaching', 'read', 'gives', 'access', 'world', 'also', 'traditionally', 'gendered', 'male'], ['used', 'metaphor', 'public', 'realm', 'systems', 'circulation', 'enable', 'participation', 'public', 'realm'], ['feminized', 'jobs', 'open', 'cooking', 'cleaning'], ['excluded', 'regular', 'unable', 'participate', 'monetary', 'open', 'bank', 'vehicular', 'get', 'ride', 'social', 'asks', 'exists', 'write'], ['learning', 'grabs', 'books', 'auto', 'spirituality'], ['therefore', 'placed', 'value', 'plane', 'books'], ['organized', 'religion', 'general', 'occasionally', 'present', 'also', 'appears', 'dresser', 'camera', 'pans', 'find', 'sex'], ['acknowledged', 'moral', 'force', 'clearly', 'character', 'devoted', 'living', 'mentions', 'beginning', 'film', 'rosary', 'among', 'objects', 'lost', 'purse', 'snatching'], ['able', 'enters', 'system', 'lands', 'managerial', 'position', 'health', 'care', 'taking', 'place', 'head', 'breadwinner'], ['industrial', 'dreaming', 'products', 'require', 'others', 'enduring', 'drudgery', 'assembly', 'line', 'produce'], ['probably', 'bit', 'conventional', 'incongruous', 'come', 'least', 'wonder', 'forced', 'exec', 'suddenly', 'worried', 'lack', 'ending', 'potential', 'effect', 'bottom', 'line'], ['according', 'comfortably', 'moved', 'film', 'also', 'slightly', 'nostalgic', 'though', 'needed', 'historical', 'distance', 'really', 'analyze', 'happened', 'period'], ['highly', 'recommended', 'least', 'want', 'exercise', 'brain'], ['close', 'perfect'], ['heart', 'warming', 'film', 'two', 'people', 'find', 'help', 'one', 'another', 'overcome', 'problems', 'life'], ['life', 'never', 'learned', 'read', 'write'], ['widower', 'two', 'teenage', 'children', 'working', 'bakery', 'meets'], ['decides', 'teach', 'read', 'home', 'spare', 'time'], ['time', 'become', 'romantically', 'involved'], ['learns', 'goes', 'good', 'job', 'return', 'ask', 'marry'], ['really', 'good', 'film', 'without', 'rare', 'films'], ['good', 'film', 'round'], ['read', 'comments', 'feel', 'jump'], ['understand', 'like', 'reasons', 'evident'], ['feeling', 'regarding', 'film', 'afraid', 'travel', 'darker', 'roads', 'disappointment', 'sorrow'], ['two', 'plenty', 'reasons', 'bitter', 'yet', 'find', 'tenderness', 'comfort'], ['great', 'acting', 'could', 'make', 'work', 'without', 'becoming', 'emotional', 'sentimental', 'sappy'], ['really', 'became', 'interested', 'people', 'overwhelming', 'humanity', 'given', 'strong', 'performances'], ['every', 'reason', 'dislike', 'era', 'personal', 'feelings', 'fabulous', 'role'], ['superb', 'man', 'whose', 'intelligence', 'goodness', 'begins', 'fail', 'world', 'indifferent', 'abilities'], ['first', 'seen', 'using', 'tenderness', 'rather', 'toughness', 'sell', 'character', 'really', 'like'], ['film', 'big', 'surprise', 'first', 'viewed', 'look', 'forward', 'seeing'], ['like', 'adult', 'comedy', 'like', 'nearly', 'similar', 'format', 'small', 'adventures', 'three', 'teenage', 'girls'], ['given', 'exploding', 'sweets', 'behaved', 'like', 'think', 'good', 'leader'], ['also', 'small', 'stories', 'going', 'teachers', 'school'], ['idiotic'], ['nervous', 'teacher', 'many', 'others'], ['cast', 'also'], ['know', 'came', 'good'], ['show', 'triumph', 'human', 'spirit'], ['struggle', 'become', 'literate', 'realize', 'potential'], ['find', 'courage', 'love', 'becoming', 'widow'], ['beauty', 'movie', 'dance', 'starting', 'skills', 'courage', 'completely', 'trust', 'move'], ['sense', 'nicely', 'gives', 'good', 'view', 'life', 'often', 'thus', 'credible'], ['reviewers', 'found', 'characters', 'rendered', 'consistent', 'whole', 'picture'], ['supporting', 'cast', 'also', 'carefully', 'chosen', 'add', 'depth', 'character', 'main', 'characters', 'get', 'added', 'meaning', 'supporting', 'performances'], ['excellent', 'movie'], ['best', 'thing', 'take'], ['nothing', 'short', 'brilliant'], ['scripted', 'perfectly', 'searing', 'parody', 'students', 'teachers', 'leaves', 'literally', 'rolling', 'laughter'], ['witty', 'sharp'], ['characters', 'superbly', 'caricatured', 'cross', 'section', 'society'], ['escapades', 'three', 'want', 'better', 'show', 'shy', 'away', 'parodying', 'every', 'imaginable', 'subject'], ['correctness', 'flies', 'window', 'every', 'episode'], ['enjoy', 'shows', 'afraid', 'poke', 'fun', 'every', 'taboo', 'subject'], ['stage', 'people', 'actors', 'something', 'like'], ['hell', 'said', 'theatre', 'stopped', 'orchestra', 'even', 'theatre', 'audience', 'participants', 'theatrical', 'including', 'story', 'film', 'grand', 'experiment', 'story', 'needs', 'needs', 'active'], ['bring', 'story', 'sometimes', 'story'], ['one', 'mean', 'said'], ['show', 'preserved', 'experimental', 'theatre', 'movement', 'origins'], ['genuinely', 'darkly', 'even', 'often', 'deeply', 'disturbing', 'tale', 'personal', 'serious', 'morality', 'tale', 'even', 'relevant', 'time', 'wants', 'outlaw', 'gay', 'marriage', 'trashing'], ['story', 'though', 'love', 'sex', 'conform', 'social', 'norms', 'therefore', 'must', 'removed', 'violence', 'hate'], ['tells', 'story', 'man', 'falls', 'love', 'like', 'great', 'really', 'something', 'bigger', 'stifling', 'conformity'], ['stage', 'version', 'international', 'acclaim', 'original', 'toured'], ['others', 'influenced', 'almost', 'theatre', 'came'], ['preserved', 'show', 'pretty', 'much', 'originally', 'original', 'cast', 'original', 'also', 'directed'], ['studio', 'film', 'deeply', 'wildly', 'imaginative', 'piece', 'storytelling', 'never', 'forget'], ['might', 'change', 'way', 'see', 'world'], ['came', 'middle', 'film', 'idea', 'credits', 'even', 'title', 'till', 'looked', 'see', 'received', 'mixed', 'reception', 'commentators'], ['positive', 'side', 'regarding', 'film', 'one', 'thing', 'really', 'caught', 'attention', 'beautiful', 'sensitive', 'score', 'written', 'style'], ['surprise', 'great', 'discovered', 'score', 'written', 'none'], ['written', 'sensitive', 'poignant', 'scores', 'one', 'usually', 'associates', 'name'], ['opinion', 'written', 'movie', 'surpasses', 'anything', 'ever', 'heard', 'sensitivity', 'fully', 'keeping', 'tender', 'lovely', 'plot', 'movie'], ['another', 'recent', 'score', 'shows', 'still', 'wit', 'sophistication'], ['like', 'education', 'movies', 'like', 'one', 'young', 'charges', 'etc'], ['tell', 'necessary', 'story', 'intellectual', 'spiritual', 'story', 'told', 'often', 'enough'], ['one', 'excellent', 'addition', 'genre'], ['story', 'movie', 'focuses', 'lives', 'blue', 'collar', 'people', 'finding', 'new', 'life', 'thru', 'new', 'love'], ['acting', 'good', 'film', 'fails', 'editing'], ['average', 'best'], ['film', 'enjoyed', 'fans', 'people', 'love', 'middle', 'age', 'love', 'stories', 'wiser', 'cautious', 'level'], ['would', 'also', 'interesting', 'people', 'interested', 'subject', 'matter', 'regarding', 'illiteracy'], ['like', 'tremendously', 'admire', 'acting'], ['great', 'movie'], ['always', 'fan', 'work', 'delicate', 'strong', 'time'], ['ability', 'make', 'every', 'role', 'portrays', 'acting', 'gold'], ['gives', 'great', 'performance', 'film', 'great', 'scene', 'take', 'father', 'home', 'elderly', 'people', 'care', 'anymore', 'break', 'heart'], ['really', 'recommend', 'film', 'great', 'cinematic', 'say', 'see', 'much', 'bette', 'acting', 'anywhere'], ['good', 'drama', 'although', 'appeared', 'blank', 'areas', 'leaving', 'viewers', 'fill', 'action'], ['imagine', 'life', 'way', 'someone', 'neither', 'read', 'write'], ['film', 'simply', 'smacked', 'real', 'wife', 'suddenly', 'sole', 'relatives', 'troubled', 'child', 'gets', 'knocked', 'drops', 'jackass', 'husband', 'takes', 'nest', 'egg', 'buys', 'beer'], ['thumbs'], ['romantic', 'drama', 'director', 'unbelievable', 'yet', 'moments', 'pleasure', 'due', 'mostly', 'charisma', 'stars'], ['widow', 'move', 'illiterate', 'probably', 'guess', 'rest'], ['novel', 'better', 'verges', 'editing', 'still', 'fantasy'], ['overtures', 'serious', 'issues', 'illiteracy', 'angle', 'ensuing', 'love', 'real', 'though', 'characters', 'intentionally', 'bit', 'colorless', 'leads', 'toned', 'interesting', 'degree'], ['finale', 'pure', 'cynics', 'find', 'difficult', 'two', 'characters', 'deserve', 'happy', 'ending', 'picture', 'really', 'satisfying', 'way']]\n"
     ]
    }
   ],
   "source": [
    "#load the vocabulary\n",
    "vocab = load_doc(\"vocab.txt\")\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "\n",
    "def cleanedlines(doc, vocab):\n",
    "    clean_lines = list ()\n",
    "    lines = doc.split(\".\")\n",
    "    for line in lines :\n",
    "        tokens = line.split()\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "\n",
    "def entire_folder2(directories , vocab ) :\n",
    "    lines = list()\n",
    "    for directory in directories:\n",
    "        for filename in listdir(directory) :#list  dir gives the list \n",
    "            path = directory + '/' + filename\n",
    "            doc = load_doc(path)\n",
    "            tokens = cleanedlines(doc, vocab)\n",
    "            lines+=tokens\n",
    "    sentences = [x for x in lines if x != []]\n",
    "    return sentences\n",
    "#'C:/Users/pc/Downloads/aclImdb/train/neg','C:/Users/pc/Downloads/aclImdb/train/pos','C:/Users/pc/Downloads/aclImdb/train/try'\n",
    "directory =  ['C:/Users/pc/Downloads/aclImdb/train/try']\n",
    "lines = entire_folder2(directory , vocab )\n",
    "#print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6abafd1a71ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m \u001b[0mE\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-6abafd1a71ed>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(lines, N, learning_rate, num_iterations)\u001b[0m\n\u001b[0;32m    111\u001b[0m           \u001b[1;31m#  L = z[j]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mj\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus2io\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-6abafd1a71ed>\u001b[0m in \u001b[0;36mcorpus2io\u001b[1;34m(corpus_tokenized, V, window_size)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcorpus2io\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_tokenized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus_tokenized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;31m#L=z\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mcontexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer()    #doesn't even have a dictionary\n",
    "    tokenizer.fit_on_texts(corpus) ## fit the tokenizer on the documents\n",
    "    corpus_tokenized = tokenizer.texts_to_sequences(corpus)#sequence encode\n",
    "    V = len(tokenizer.word_index)\n",
    "    return corpus_tokenized, V\n",
    "\"\"\"def length(k):\n",
    "    N=len(k)\n",
    "    n = list()\n",
    "    for i in range(N):\n",
    "        n.append(len(k[i]))\n",
    "    return n\"\"\"\n",
    "\n",
    "\"\"\"k,r = tokenize(lines)\n",
    "for i in k:\n",
    "    for j in i:\n",
    "        print(j)\"\"\"\n",
    "\n",
    "#DOUBT : DIDINOT UNDERSTAND THE CODE\n",
    "def to_categorical(y, num_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape                        \n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:  \n",
    "        num_classes = np.max(y) + 1 #doubt :\n",
    "    n = y.shape[0] \n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical\n",
    "\n",
    "            \n",
    "def corpus2io(corpus_tokenized, V, window_size):\n",
    "    for words in corpus_tokenized:\n",
    "        L = len(words)\n",
    "        #L=z\n",
    "        contexts = []\n",
    "        labels = []\n",
    "        s = index - window_size\n",
    "        e = index + window_size + 1\n",
    "        contexts.append([words[i]-1 for i in range(s, e) if 0 <= i < L and i != index])\n",
    "        labels.append(word-1)\n",
    "        x = np_utils.to_categorical(contexts, V)\n",
    "        y = np_utils.to_categorical(labels, V)\n",
    "        yield (x, y.ravel())\n",
    "\n",
    "\n",
    "def softmax(u,W1,W2):\n",
    "   # e_x = np.exp(x - np.max(x))\n",
    "    y = np.exp(u)/ np.dot(W2.T,np.sum(W1.T, axis = 1)) # v x 1\n",
    "  #  y = np.exp(u)/(np.sum(np.exp(np.matmul(W1,W2),axis = 1)))\n",
    "    return y#V x 1\n",
    "    #return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    \n",
    "def initialize(N,V):\n",
    "    W1 = np.random.randn(V,N) * 0.01\n",
    "    W2 = np.random.randn(V,N) * 0.01\n",
    "    return W1,W2\n",
    "\n",
    "def cbow( context ,y, W1, W2,learning_rate):\n",
    "       \n",
    "        # context is 'x' from tokenizer, it is a c x V matrix\n",
    "        # label is 'y' from tokenizer, it is a 1 x V matrix\n",
    "        x = np.matrix(np.mean(context, axis=0)) #returns matrix from an array \n",
    "        \n",
    "        # x is a 1xV matrix\n",
    "        # W1 is a VxN matrix\n",
    "        # h is a N x 1 matrix\n",
    "        h = np.matmul(W1.T, x.T)\n",
    "         # u is a V x 1 matrix\n",
    "        u = np.matmul(W2.T, h)\n",
    "        # W2 is an N x V matrix\n",
    "        # y_pred is a V x 1 matrix\n",
    "        y_pred = softmax(u,W1,W2)\n",
    "        \n",
    "        # e is a V x 1 matrix\n",
    "        e = -label.T + y_pred\n",
    "        # h is N x 1 and e is V x 1 so dW2 is N x V\n",
    "        dW2 = np.outer(h, e)\n",
    "        # x.T is a V x 1 matrix, W2e is a Nx1 so dW1 this is V x N\n",
    "        dW1 = np.outer(x.T, np.matmul(W2, e))\n",
    "        \n",
    "        \n",
    "\n",
    "        new_W1 = W1 - learning_rate * dW1\n",
    "        new_W2 = W2 - learning_rate * dW2\n",
    "\n",
    "        # label is a 1xV matrix so label.T is a Vx1 matrix\n",
    "        loss += -float(u[label.T == 1]) + np.log(np.sum(np.exp(u)))\n",
    "\n",
    "        return new_W1, new_W2, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model(lines,N,learning_rate,num_iterations):\n",
    "    encoded_train_docs, V = tokenize(lines)\n",
    "   # z = k(encoded_train_docs)\n",
    "    W1, W2 = initialize(V, N)\n",
    "    cost = []\n",
    "    for num in range(num_iterations):\n",
    "        j=0\n",
    "        losses = 0\n",
    "        for i in encoded_train_docs:\n",
    "          #  L = z[j]\n",
    "            j+=1\n",
    "            x,y = corpus2io(i,V,window_size = 1)\n",
    "            W1,W2,loss = cbow(x,y,W1, W2,learning_rate)\n",
    "            losses += loss\n",
    "        if num%100 == 0:\n",
    "            cost.append(losses/j)\n",
    "            \n",
    "    plt.plot(np.squeeze(cost))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "                \n",
    "    return W1.T \n",
    "\n",
    "E  = model (lines ,100,0.025,1500)\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  tsne.py\n",
    "#\n",
    "# Implementation of t-SNE in Python. The implementation was tested on Python\n",
    "# 2.7.10, and it requires a working installation of NumPy. The implementation\n",
    "# comes with an example on the MNIST dataset. In order to plot the\n",
    "# results of this example, a working installation of matplotlib is required.\n",
    "#\n",
    "# The example can be run by executing: `ipython tsne.py`\n",
    "#\n",
    "#\n",
    "#  Created by Laurens van der Maaten on 20-12-08.\n",
    "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
    "\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "\n",
    "def Hbeta(D=np.array([]), beta=1.0):\n",
    "    \"\"\"\n",
    "        Compute the perplexity and the P-row for a specific value of the\n",
    "        precision of a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    P = np.exp(-D.copy() * beta)\n",
    "    sumP = sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(D * P) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "\n",
    "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a binary search to get P-values in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    (n, d) = X.shape\n",
    "    sum_X = np.sum(np.square(X), 1)\n",
    "    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "    P = np.zeros((n, n))\n",
    "    beta = np.ones((n, 1))\n",
    "    logU = np.log(perplexity)\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    for i in range(n):\n",
    "\n",
    "        # Print progress\n",
    "        if i % 500 == 0:\n",
    "            print(\"Computing P-values for point %d of %d...\" % (i, n))\n",
    "\n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        betamin = -np.inf\n",
    "        betamax = np.inf\n",
    "        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n",
    "        (H, thisP) = Hbeta(Di, beta[i])\n",
    "\n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while np.abs(Hdiff) > tol and tries < 50:\n",
    "\n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i].copy()\n",
    "                if betamax == np.inf or betamax == -np.inf:\n",
    "                    beta[i] = beta[i] * 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2.\n",
    "            else:\n",
    "                betamax = beta[i].copy()\n",
    "                if betamin == np.inf or betamin == -np.inf:\n",
    "                    beta[i] = beta[i] / 2.\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2.\n",
    "\n",
    "            # Recompute the values\n",
    "            (H, thisP) = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "\n",
    "        # Set the final row of P\n",
    "        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n",
    "\n",
    "    # Return final P-matrix\n",
    "    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 / beta)))\n",
    "    return P\n",
    "\n",
    "\n",
    "def pca(X=np.array([]), no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - np.tile(np.mean(X, 0), (n, 1))\n",
    "    (l, M) = np.linalg.eig(np.dot(X.T, X))\n",
    "    Y = np.dot(X, M[:, 0:no_dims])\n",
    "    return Y\n",
    "\n",
    "\n",
    "def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    X = pca(X, initial_dims).real\n",
    "    (n, d) = X.shape\n",
    "    max_iter = 1000\n",
    "    initial_momentum = 0.5\n",
    "    final_momentum = 0.8\n",
    "    eta = 500\n",
    "    min_gain = 0.01\n",
    "    Y = np.random.randn(n, no_dims)\n",
    "    dY = np.zeros((n, no_dims))\n",
    "    iY = np.zeros((n, no_dims))\n",
    "    gains = np.ones((n, no_dims))\n",
    "\n",
    "    # Compute P-values\n",
    "    P = x2p(X, 1e-5, perplexity)\n",
    "    P = P + np.transpose(P)\n",
    "    P = P / np.sum(P)\n",
    "    P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "    P = np.maximum(P, 1e-12)\n",
    "\n",
    "    # Run iterations\n",
    "    for iter in range(max_iter):\n",
    "\n",
    "        # Compute pairwise affinities\n",
    "        sum_Y = np.sum(np.square(Y), 1)\n",
    "        num = -2. * np.dot(Y, Y.T)\n",
    "        num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n",
    "        num[range(n), range(n)] = 0.\n",
    "        Q = num / np.sum(num)\n",
    "        Q = np.maximum(Q, 1e-12)\n",
    "\n",
    "        # Compute gradient\n",
    "        PQ = P - Q\n",
    "        for i in range(n):\n",
    "            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n",
    "\n",
    "        # Perform the update\n",
    "        if iter < 20:\n",
    "            momentum = initial_momentum\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n",
    "                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n",
    "        gains[gains < min_gain] = min_gain\n",
    "        iY = momentum * iY - eta * (gains * dY)\n",
    "        Y = Y + iY\n",
    "        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n",
    "\n",
    "        # Compute current value of cost function\n",
    "        if (iter + 1) % 10 == 0:\n",
    "            C = np.sum(P * np.log(P / Q))\n",
    "            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            P = P / 4.\n",
    "\n",
    "    # Return solution\n",
    "    return Y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   # print(\"Run Y = tsne.tsne(X, no_dims, perplexity) to perform t-SNE on your dataset.\")\n",
    "   # print(\"Running example on 2,500 MNIST digits...\")\n",
    "    X = np.loadtxt(\"mnist2500_X.txt\")\n",
    "    labels = np.loadtxt(\"mnist2500_labels.txt\")\n",
    "    Y = tsne(X, 2, 50, 20.0)\n",
    "    pylab.scatter(Y[:, 0], Y[:, 1], 20, labels)\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200, 3]\n",
      "3\n",
      "[1, 2]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = [[1,200,3],[1,2]]\n",
    "for i in a:\n",
    "    print(i)\n",
    "    print(len(i))\n",
    "#len(a[0])\n",
    "\n",
    "#[len(x) for x in a[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stupid\n"
     ]
    }
   ],
   "source": [
    "print(\"stupid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
